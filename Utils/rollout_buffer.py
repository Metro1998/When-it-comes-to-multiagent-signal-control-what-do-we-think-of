"""
reference:
https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py

Author:Metro
date:2023.1.10
"""

import numpy as np
import torch
from Utils.util import discount_cumsum


class PPOBuffer:
    """
    A buffer for storing trajectories generated by (H)PPO agents interacting with
    parallel sub-environments, and using Generalized Advantage Estimation (GAE-Lambda)
    for calculating the advantages of obs-action pairs.
    """

    def __init__(self, num_steps, num_envs, num_agents, obs_dim, sequence_dim, act_dim, minus_inf, device):
        self.obs_buf = np.zeros((num_steps, num_envs, num_agents, obs_dim), dtype=np.float32)
        # A buffer containing the history of signal before
        self.obs_sequence_buf = np.zeros((num_steps, num_envs, num_agents, sequence_dim), dtype=np.int64)
        self.act_dis_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.int64)
        self.act_con_buf = np.zeros((num_steps, num_envs, num_agents, act_dim), dtype=np.float32)
        self.logp_dis_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.logp_con_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.adv_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.rew_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.ret_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.val_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        # to indicate whether one specific agent is on its atomic timestep
        self.flag_buf = np.zeros((num_steps, num_envs, num_agents), dtype=np.float32)
        self.minus_inf = minus_inf
        self.device = device
        self.num_steps, self.num_envs, self.num_agents = num_steps, num_envs, num_agents
        self.obs_dim, self.act_dim = obs_dim, act_dim
        self.ptr, self.path_start_dix, self.max_size = 0, 0, num_steps

    def store_trajectories(self, obs, obs_sequence, act_dis, act_con, rew, val, logp_dis, logp_con):
        """
`       Append one timestep of agent-environment interaction to the buffer.
        ### Inputs are batch of num_envs * num_agents ###
        """
        assert self.ptr < self.max_size
        self.obs_buf[self.ptr] = obs
        self.obs_sequence_buf[self.ptr] = obs_sequence
        self.act_dis_buf[self.ptr] = act_dis
        self.act_con_buf[self.ptr] = act_con
        self.logp_dis_buf[self.ptr] = logp_dis
        self.logp_con_buf[self.ptr] = logp_con
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.ptr += 1

    # def compute_returns_and_advantages(self, last_val=0):
    #     """
    #     Call this when one gets cut off by an epoch ending.
    #     This uses rewards and value estimates from
    #     the whole trajectory to compute advantage estimates with GAE-Lambda,
    #     as well as compute the rewards-to-go for each obs, to use as
    #     the targets for the value function.
    #     :param last_val: should be next_obs
    #     :return:
    #     """
    #
    #     rews = np.append(self.rew_buf[:self.ptr], last_val)  # just to fill up the space
    #     vals = np.append(self.val_buf[:self.ptr], last_val)
    #
    #     # the next two lines implement GAE-Lambda advantage calculation
    #     deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]
    #     self.adv_buf[:self.ptr] = discount_cumsum(deltas, self.gamma * self.lam)
    #
    #     # the next line computes rewards-to-go, to be targets for the value function
    #     self.ret_buf[:self.ptr] = discount_cumsum(rews, self.gamma)[:-1]

    def get(self):
        """
        Call this at the end of a rollout round to retrieve the full infomation.
        :return:
        """
        assert self.ptr == self.max_size  # buffer has to be full before you can get

        # Critic will get the centralized observation,
        # Sequence observation is not considered here.
        # (num_steps, num_envs, num_agents, obs_dim) --> (num_envs, num_steps, num_agents * obs_dim)
        cent_obs_buf = self.obs_buf[:self.ptr].transpose(1, 0, 2, 3).rehape(self.num_envs, self.num_steps, -1)
        # (num_steps, num_envs, num_agents) --> --> (num_envs, num_steps, num_agents)
        rew_buf = self.rew_buf[:self.ptr].transpose(1, 0, 2)

        # (num_steps, num_envs, num_agents, obs_dim) --> (num_agents, num_envs, num_steps, obs_dim)
        obs_buf = self.obs_buf[:self.ptr].transpose(2, 1, 0, 3)
        obs_sequence_buf = self.obs_sequence_buf[:self.ptr].transpose(2, 1, 0, 3)
        # (num_steps, num_envs, num_agents) --> (num_agents, num_envs, num_steps)
        # if agent_i executes on the timestep_j, then it will get obs and obs_sequence, otherwise self.minus.
        flag_buf = self.flag_buf[:self.ptr].tranpose(2, 1, 0)

        obs_agent_buf = np.where(np.expand_dims(flag_buf, axis=-1), obs_buf, self.minus_inf)
        obs_sequence_agent_buf = np.where(np.expand_dims(flag_buf, axis=-1), obs_sequence_buf, self.minus_inf)
        act_dis_buf = self.act_dis_buf[:self.ptr].transpose(2, 1, 0)
        act_con_buf = self.act_con_buf[:self.ptr].transpose(2, 1, 0, 3)
        logp_dis_buf = self.logp_dis_buf[:self.ptr].transpose(2, 1, 0)
        logp_con_buf = self.logp_con_buf[:self.ptr].transpose(2, 1, 0)

        return cent_obs_buf, rew_buf, obs_agent_buf, obs_sequence_agent_buf, act_dis_buf, act_con_buf, logp_dis_buf, logp_con_buf

    def filter(self):
        """
        Get the obs's mean and std for next update cycle.
        :return:
        """
        obs = self.obs_buf[:self.ptr]

        return obs.mean(axis=0), obs.std(axis=0)

    def clear(self):
        self.ptr, self.path_start_dix = 0, 0


if __name__ == "__main__":
    buffer = PPOBuffer(num_steps=2, num_envs=4, num_agents=3, obs_dim=2, act_dim=1, gamma=0.99, lam=0.8, device="cpu")
    buffer.active_buf = np.array([[
        [0, 1, 0],
        [1, 1, 1],
        [0, 0, 0],
        [1, 0, 1]],
        [[1, 0, 0],
         [1, 0, 1],
         [0, 1, 0],
         [1, 1, 0]
         ]])
    print(buffer.active_buf.shape)
    buffer.active_buf = np.reshape(buffer.active_buf, (2, 4, 3, 1))
    print(buffer.active_buf)
    buffer.obs_buf = np.array([
        [
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]]
        ],
        [
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]],
            [[1.2, 2.1], [1.5, 1.7], [1.5, 1.7]]
        ]
    ])
    buffer.obs_buf = buffer.active_buf * buffer.obs_buf

    tmp = np.reshape(buffer.obs_buf, (-1, 2))
    print(tmp)
